{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce876568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\docs\\Democratize computer vision defect detection for manufacturing quality using no-code machine learning with Amazon SageMaker Canvas _ AWS Machine Learning Blog.txt\n",
      "..\\..\\docs\\Helping Customers Modernize Their Cloud Infrastructure Using the AWS Well-Architected Framework with Comprinno _ Comprinno Technologies Case Study _ AWS.txt\n",
      "..\\..\\docs\\Isetan Mitsukoshi System Solutions seamlessly migrates databases to Amazon Aurora using Amazon DMA _ Isetan Mitsukoshi System Solutions Case Study _ AWS.txt\n",
      "..\\..\\docs\\NTT DOCOMO builds a new data analysis platform for 9000 workers with AWS attracting 13 times more users and invigorating data use _ NTT Docomo Case Study _ AWS.txt\n",
      "..\\..\\docs\\Transforming fleet telematics into predictive analytics with Capgeminis Trusted Vehicle and AWS IoT FleetWise _ The Internet of Things on AWS  Official Blog.txt\n",
      "..\\..\\docs\\WebBeds uses Amazon EC2 Spot Instances to save its business amid a reduction in travel worldwide and reduce costs up to 64 percent. _ WebBeds Case Study _ AWS.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "docs = []\n",
    "for file in Path(\"../../docs\").glob(\"*.txt\"):\n",
    "    try:\n",
    "        with open(file, encoding=\"utf-8\", errors=\"ignore\") as file_handler:\n",
    "            docs.append(file_handler.read())\n",
    "    except FileNotFoundError:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8bce2c-e123-498a-a5f2-cefffd17fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models, QdrantClient\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0e4be5-7518-4458-bf47-6913ef9a72a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SentenceTransformer('all-MiniLM-L6-v2') # Model to create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5efa031d-b18a-4db1-9c34-9989a15c822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vector database client\n",
    "qdrant = QdrantClient(\":memory:\") # Create in-memory Qdrant instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c03be93-a076-425e-8df1-5a8b6367e558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create collection to store wines\n",
    "collection_name = \"aws-case-studies-and-blogs\"\n",
    "vector_params = models.VectorParams(\n",
    "    size=encoder.get_sentence_embedding_dimension(),  # Vector size is defined by the model\n",
    "    distance=models.Distance.COSINE,\n",
    ")\n",
    "\n",
    "if qdrant.collection_exists(collection_name):\n",
    "    qdrant.delete_collection(collection_name)\n",
    "\n",
    "qdrant.create_collection(collection_name=collection_name, vectors_config=vector_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9dc7dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize!\n",
    "vectors = encoder.encode(docs)\n",
    "qdrant.upload_points(\n",
    "    collection_name=collection_name,\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=idx,\n",
    "            vector=vector.tolist(),\n",
    "            payload={\"text\": doc},\n",
    "        )\n",
    "        for idx, (vector, doc) in enumerate(\n",
    "            zip(vectors, docs)\n",
    "        )  # data is the variable holding all the wines\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f23bc999",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Suggest me an amazing Malbec wine from Argentina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68c9bff5-db38-4a98-b542-cd173af11b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '\\n\\n\\n\\nAWS Machine Learning Blog\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroducing popularity tuning for Similar-Items in Amazon Personalize\\n\\n\\n\\n        by \\n       \\nJulia Clark\\n, \\n       \\nBranislav Kveton\\n, \\n       \\nNihal Harish\\n, and \\n       \\nYifei Ma\\n | on \\n       \\n08 JUN 2023\\n | in \\n       \\nAmazon Machine Learning\\n, \\nAmazon Personalize\\n | \\n       \\nPermalink\\n | \\n       \\n Comments\\n | \\n       \\n\\xa0Share\\n\\n\\n\\n\\n\\n\\n \\n \\n\\n\\n \\n \\n\\n\\n \\n \\n\\n\\n \\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAmazon Personalize\\n now enables popularity tuning for its \\nSimilar-Items recipe\\n (\\naws-similar-items\\n). Similar-Items generates recommendations that are similar to the item that a user selects, helping users discover new items in your catalog based on the previous behavior of all users and item metadata. Previously, this capability was only available for \\nSIMS\\n, the other \\nRelated_Items\\n recipe within Amazon Personalize.\\n\\n\\nEvery customer’s item catalog and the way that users interact with it are unique to their business. When recommending similar items, some customers may want to place more emphasis on popular items because they increase the likelihood of user interaction, while others may want to de-emphasize popular items to surface recommendations that are more similar to the selected item but are less widely known. This launch gives you more control over the degree to which popularity influences Similar-Items recommendations, so you can tune the model to meet your particular business needs.\\n\\n\\nIn this post, we show you how to tune popularity for the Similar-Items recipe. We specify a value closer to zero to include more popular items, and specify a value closer to 1 to place less emphasis on popularity.\\n\\n\\nExample use cases\\n\\n\\nTo explore the impact of this new feature in greater detail, let’s review two examples. [1]\\n\\n\\nFirst, we used the Similar-Items recipe to find recommendations similar to Disney’s 1994 movie The Lion King (\\nIMDB record\\n). When the popularity discount is set to 0, Amazon Personalize recommends movies that have a high frequency of occurrence (are popular). In this example, the movie Seven (a.k.a. Se7en), which occurred 19,295 times in the dataset, is recommended at rank 3.0.\\n\\n\\n\\n\\nBy tuning the popularity discount to a value of 0.4 for The Lion King recommendations, we see that the rank of the movie Seven drops to 4.0. We also see movies from the Children genre like Babe, Beauty and the Beast, Aladdin, and Snow White and the Seven Dwarfs get recommended at a higher rank despite their lower overall popularity in the dataset.\\n\\n\\n\\n\\nLet’s explore another example. We used the Similar-Items recipe to find recommendations similar to Disney and Pixar’s 1995 movie Toy Story (\\nIMDB record\\n). When the popularity discount is set to 0, Amazon Personalize recommends movies that have a high frequency occurrence in the dataset. In this example, we see that the movie Twelve Monkeys (a.k.a. 12 Monkeys), which occurred 6,678 times in the dataset, is recommended at rank 5.0.\\n\\n\\n\\n\\nBy tuning the popularity discount to a value of 0.4 for Toy Story recommendations, we see that the rank of the Twelve Monkeys is no longer recommended in the top 10. We also see movies from the Children genre like Aladdin, Toy Story 2, and A Bug’s Life get recommended at a higher rank despite their lower overall popularity in the dataset.\\n\\n\\n\\n\\nPlacing greater emphasis on more popular content can help increase likelihood that users will engage with item recommendations. Reducing emphasis on popularity may surface recommendations that seem more relevant to the queried item, but may be less popular with users. You can tune the degree of importance placed on popularity to meet your business needs for a specific personalization campaign.\\n\\n\\nImplement popularity tuning\\n\\n\\nTo tune popularity for the Similar-Items recipe, configure the \\npopularity_discount_factor\\n hyperparameter via the \\nAWS Management Console\\n, the AWS SDKs, or the \\nAWS Command Line Interface\\n (AWS CLI).\\n\\n\\nThe following is sample code setting the popularity discount factor to 0.5 via the AWS SDK:\\n\\n\\n\\n\\n{\\n\\tresponse = personalize.create_solution(\\n\\t\\tname=\"movie_lens-with-popularity-discount-0_5\".\\n\\t\\trecipeARN=\"arn:aws:personalize:::recipe/aws-similar-items\",\\n\\t\\tdatasetGroupArn=dsg_arn,\\n\\t\\tsolutionConfig={\\n\\t\\t\\t\"algorithmHyperParameters\" : {\\n\\t\\t\\t\\t# set the preferred value of popularity discount here\\n\\t\\t\\t\\t\"popularity_discount_factor\" : \"0.50\"\\n\\t\\t\\t}\\n\\t\\t}\\n\\t]\\n}\\n\\n\\n\\n\\nThe following screenshot shows setting the popularity discount factor to 0.3 on the Amazon Personalize console.\\n\\n\\n\\n\\nConclusion\\n\\n\\nWith popularity tuning, you can now further refine the Similar-Items recipe within Amazon Personalize to control the degree to which popularity influences item recommendations. This gives you greater control over defining the end-user experience and what is included or excluded in your Similar-Items recommendations.\\n\\n\\nFor more details on how to implement popularity tuning for the Similar-Items recipe, refer to \\ndocumentation\\n.\\n\\n\\nReferences\\n\\n\\n[1] Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=\\nhttp://dx.doi.org/10.1145/2827872\\n\\n\\n\\n\\nAbout the Authors\\n\\n\\nJulia McCombs Clark\\n is a \\xa0Sr. Technical Product Manager on the Amazon Personalize team.\\n\\n\\nNihal Harish\\n is a Software Development Engineer on the Amazon Personalize team.\\n\\n\\nYifei Ma\\n is a Senior Applied Scientist at AWS AI Labs working on recommender systems. His research interests lie in active learning, sequential modeling, and online decision making.\\n\\n\\nBranislav Kveton\\n is a Principal Scientist at AWS AI Labs. He proposes, analyzes, and applies algorithms that learn incrementally, run in real time, and converge to near optimal solutions as the number of observations increases.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nComments\\n\\n\\n\\n\\nView Comments\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Resources\\n\\n\\n\\n\\n\\n\\nGetting Started\\n\\n\\nWhat\\'s New\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Blog Topics\\n\\n\\n\\n\\n\\n\\n\\n\\nAmazon Comprehend\\n\\n\\nAmazon Kendra\\n\\n\\nAmazon Lex\\n\\n\\nAmazon Polly\\n\\n\\nAmazon Rekognition\\n\\n\\nAmazon SageMaker\\n\\n\\nAmazon Textract\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Follow\\n\\n\\n\\n\\n\\n\\n \\xa0Twitter\\n\\n\\n \\xa0Facebook\\n\\n\\n \\xa0LinkedIn\\n\\n\\n \\xa0Twitch\\n\\n\\n \\xa0Email Updates\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'} score: 0.2029485806511272\n",
      "{'text': \"With the recent rise of rival music services, Anghami recognized the growing significance of guiding customers towards the artists and content that align with their preferences. This became even more crucial given the extensive and expanding collection of Arabic and international music available on the platform. These music-recommendation features attract new customers, and foster greater user loyalty. The company has observed that users spend more time on the site when presented with additional song recommendations.  Anghami's previous solution for generating recommendations used legacy code that made it difficult for its team to expand its functionality. Anghami decided to create a new, cloud-native solution on AWS. The new platform eliminated the liability of maintaining old code, and freed up more time for engineers to build new features and capabilities for customers. It also meant they could take advantage of versatile tools such as Amazon OpenSearch Service, which makes it easy to perform interactive log analytics, real-time application monitoring, and website searches.\\n Amazon OpenSearch Service\\nFrançais\\nOutcome: Owning Audio Content and Delighting Customers Using AWS \\n Amazon S3\\nEspañol\\nThe company aimed to develop a cutting-edge recommendations platform that could scale to handle its expanding user-base, while facilitating the creation of novel features and services for its customers.\\n                \\nAnghami is a music-streaming service based in Abu Dhabi. It serves approximately 70 million users in Europe, the Middle East and North Africa (MENA), and the US, giving them access to more than 72 million songs and podcasts. Over the past 10 years, it grew from a homegrown start-up into the first Arab technology company to be listed on the Nasdaq stock exchange in February 2022.   Anghami sets itself apart from competitors by helping customers find suitable audio content through personalized recommendations. When its previous technology platform proved difficult to maintain and develop new features for, it turned to Amazon Web Services (AWS). The company built a new platform on AWS that uses machine learning (ML) to generate recommendations. It can now quickly surface relevant content for users, attract top tech talent, rapidly develop new features that enrich customer experience, and support future product innovation. \\nOpportunity: Reducing Technology Risk and Building a Platform for Innovation \\n日本語\\n Amazon SageMaker\\n 2023\\nAn AWS customer since its inception, Anghami reached out to AWS solution architects to investigate its technology options based on its future plans. After several in-depth workshops, they came up with a new architecture that is simple, powerful, and easy to maintain and develop on.\\nWithin 6 months of the initial architecture workshops with AWS, Anghami launched its cloud-based recommendations engine for its growing catalog of songs and podcasts. The service’s recommendation platform now runs on Amazon OpenSearch Service. Anghami stores its user behavior data and audio content on Amazon Simple Storage Service (Amazon S3), object storage built to retrieve any amount of data from anywhere.  To run its large data workloads, the company uses Amazon EMR, which easily runs and scales Apache Spark, Hive, Presto, and other big workloads. These workloads include training nearly a decade’s worth of customer data that has been collected from millions of customers using the streaming music service daily. To train the machine learning models that produce music recommendations, Anghami uses Amazon SageMaker, which helps to build, train, and deploy ML models.\\n한국어\\nOverview | Opportunity | Solution | Outcome | AWS Services Used \\nLearn how »\\xa0\\n \\nKevin Williams Vice President (VP) of Machine Learning, Anghami \\nAnghami plans to continue growing its audio catalog and expanding its user base in the Middle East and beyond. “We want to own audio in the regions we operate, for podcasts, audiobooks, and music,” adds Williams. “Using AWS, we have everything we need to accomplish that. Our platform is flexible, reliable, scalable, and easy to maintain, so we can spend our efforts on valuable tasks that benefit customers instead of maintenance.”\\n Get Started\\n                  Organizations of all sizes use AWS to increase agility, lower costs, and accelerate innovation in the cloud. \\nBuild, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.  Learn more\\xa0»\\nOur platform is flexible, reliable, scalable, and easy to maintain, so we can spend our efforts on valuable tasks that benefit customers instead of maintenance.”  \\n AWS Services Used\\n Overview\\n中文 (繁體)\\nBahasa Indonesia\\n 10x\\n Anghami Personalizes Music Recommendations Using Amazon OpenSearch Service\\nSolution: Attracting Top Tech Talent and Developing Prototypes in Days on AWS \\nΡусский\\nOrganizations of all sizes across all industries are transforming their businesses and delivering on their missions every day using AWS. Contact our experts and start your own AWS journey today.\\nعربي\\n中文 (简体)\\n 72+ million\\nCustomer Stories / Media Entertainment / MENA \\n Amazon EMR\\n About Company\\nFounded in 2012 in Beirut, Anghami offers free and paid audio-streaming services. Its premium service provides features such as the ability to download tracks and play them offline, rewind or fast-forward music, and view lyrics.\\n AWS Customer Success Stories\\nTürkçe\\nAmazon EMR is the industry-leading cloud big data solution for petabyte-scale data processing, interactive analytics, and machine learning using open-source frameworks.\\nEnglish\\nAnghami now has a technology foundation it can build on for years to come. “I'm excited about running development sprints and discovering the best customer experiences in a timely manner,” says Williams.\\n 6 months\\nsongs and podcasts served seamlessly \\nAmazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. \\nDeutsch\\nTiếng Việt\\nAmazon OpenSearch Service makes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more. OpenSearch is an open source, distributed search and analytics suite derived from Elasticsearch.\\nAnghami can also release new music to fans almost immediately. When new tracks drop, typically on Fridays, fans can access them within a minute of the official release. With the previous solution, the tech team couldn’t quickly add a single track to the catalog. However, using OpenSearch, the team can insert and serve songs with its machine learning algorithm within moments of the song’s release. “This is an essential feature that really makes us stand out compared to our rivals,” says Williams. “It’s satisfying to build on fans’ excitement about new releases.”\\nItaliano\\nไทย\\nFounded in 2010, Anghami provides a music-streaming service in the Middle East and North Africa (MENA), Europe and the US. The company has offices in Abu Dhabi, Beirut, Cairo, Dubai, and Riyadh, and employs more than 160 people. \\nAnghami developers can now rapidly prototype new feature ideas from product teams and quickly develop queries to recommend content for users. Writing a search query and creating a prototype takes 1–2 days on AWS, as opposed to around 2 weeks on the previous system. Since launching on AWS, the team has created new functions on the service landing page that suggest artists and relevant playlists for customers to listen to, instead of just suggesting tracks.\\nBuilding its platform on AWS has reduced the company’s technology risk because it is now easier to find talented engineers and DevOps staff. “As a tech company, you’re only as good as your talent,” says Kevin Williams, Vice President (VP) of Machine Learning at Anghami. “We can quickly find candidates with OpenSearch skills and others who are motivated to learn OpenSearch because it’s a widely used technology. It's also quicker to train up technical staff, because they can access existing documentation on AWS services.”\\nLearn more\\xa0»\\nto migrate entire song database \\nfaster to develop music search queries \\nPortuguês\\n  Contact Sales\"} score: 0.1868223145900423\n",
      "{'text': 'ML models in production \\nAmazon SageMaker helps you build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows \\nFrançais\\nEspañol\\nto support other quality-assurance use cases \\n           \\xa0 \\n           Novo Nordisk has successfully built an automated pipeline to deploy ML models at scale to different edge devices. The company is turning the cartridge-counting proof of concept into a production-grade solution and will continue to build the proof of concept for its agar plate use case. These solutions will significantly impact Novo Nordisk’s efficiency, improving its time to market and reducing manual labor so that its team can focus on innovation. \\n Automates\\n日本語\\n Amazon SageMaker\\n 2023\\n About Novo Nordisk\\n  Contact Sales \\nOpportunity | Using Amazon SageMaker Pipelines to Deploy ML Models at Scale\\xa0\\n Get Started\\n한국어\\n Novo Nordisk Uses ML for Computer Vision to Optimize Pharmaceutical Manufacturing on AWS\\n \\ntime to market \\nNovo Nordisk A/S is a multinational pharmaceutical company based in Denmark. Founded in 1923, the organization makes and markets pharmaceutical products with a focus on diabetes care and hormone therapy. \\n         \\n Scales \\n           For the past 100 years, Novo Nordisk has developed innovative products to treat chronic diseases like diabetes, endocrine disorders, and rare blood conditions. More than 34 million patients use its diabetes-care products globally, and the company constantly seeks new digital technologies to optimize its processes for the benefit of its customers. It strives to get medicines to the people who need them at a faster pace and lower price while ensuring compliance. \\n AWS Services Used\\n Improves\\n中文 (繁體)\\nBahasa Indonesia\\nSolution | Automating Key Quality-Assurance Tasks with ML and Computer Vision\\xa0\\n Deploys\\nไทย\\nΡусский\\nOrganizations of all sizes across all industries are transforming their businesses and delivering on their missions every day using AWS. Contact our experts and start your own AWS journey today.\\nعربي\\nOn AWS, Novo Nordisk created an automated ML pipeline that covers all the steps involved in the ML development process, from deployment to monitoring, while optimizing for scalability, customization, cost, and traceability. It used Amazon SageMaker Pipelines, the first purpose-built continuous integration and continuous delivery service for ML, to create each specific step in the pipeline and combine them to form a complete, interconnected solution. The pipeline used prelabeled images stored in Amazon Simple Storage Service (Amazon S3)—an industry-leading object storage service. It then resizes, labels, processes, and splits the images into three datasets: training, validation, and testing. \\nAmazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance.\\n中文 (简体)\\n           “Through our engagement with the AWS team, we proved to ourselves and our company that we could take a computer-vision use case, put it into the cloud, and build a working pipeline,” says Kristensen. “And we can do it in a fast and scalable way.” \\nOutcome | Using AWS Services to Streamline the Pharmaceutical Manufacturing Line\\xa0\\n Overview\\n          \\nAmazon QuickSight powers data-driven organizations with unified business intelligence (BI) at hyperscale.  Learn more\\xa0»\\nJonas Vejlgård Kristensen  Solutions Architect, Novo Nordisk  \\nTürkçe\\nEnglish\\nNovo Nordisk A/S (Novo Nordisk) supplies nearly 50 percent of the world’s insulin. Digital technologies are critical to optimizing the company’s manufacturing operations, enhancing quality, improving yield, and decreasing costs. To this end, Novo Nordisk is using computer vision combined with machine learning (ML) to automate key tasks on manufacturing lines, like cartridge counting and anomaly detection for agar plates, to reduce manual labor. \\nAWS IoT Greengrass is an open-source edge runtime and cloud service for building, deploying, and managing device software.  Learn more\\xa0»\\n Monitors\\nOn Amazon Web Services (AWS), Novo Nordisk has created a prototyping solution that effectively trains, deploys, and monitors its ML models and manages the datasets resulting from the pipelines. Alongside the AWS team, the company has built a workflow where a robotic arm places a box full of drug cartridges on a platform; a camera rig takes images of the box; ML inference is performed using an edge device; and the final results are displayed on a dashboard powered by Amazon QuickSight, which provides unified business intelligence at hyperscale. \\nLearn how Novo Nordisk uses AWS to streamline manufacturing processes and reduce manual labor through automation. \\nDeutsch\\n AWS IoT Greengrass\\nTiếng Việt\\n Amazon S3\\nThrough our engagement with the AWS team, we proved to ourselves and our company that we could take a computer-vision use case, put it into the cloud, and build a working pipeline.\" \\nItaliano\\nCustomer Stories / Life Sciences \\nAfter the data is processed, the pipeline passes it to either model training, where it is trained with predefined parameters, or model tuning, where it is run through different parameters to find the optimal combination. Then, Novo Nordisk uses the test dataset to generate an evaluation report and determine whether the model is ready for deployment. After registering the model, it compiles the model and packages for deployment using Amazon SageMaker Edge, which makes it simple to operate ML models running on edge devices. The company also uses Amazon SageMaker Edge Manager, which provides model management for edge devices, to perform ML inference of each image. \\xa0 Next, Novo Nordisk uses AWS IoT Greengrass, an open-source edge runtime and cloud service, to deploy the ML model and serve as the core software for the edge device. “We use AWS services to optimize our ML model for a specific edge device,” says Codina. “When we have the model up and running, every time that we make a prediction, we process the data and send it to the cloud to perform model monitoring.” Novo Nordisk monitors its ML models in production using Amazon QuickSight and Amazon Timestream, a fast, scalable, and serverless time-series database. With these monitoring capabilities, it can detect any anomalies and identify inaccurate results. For example, if a hand or object is covering a box of cartridges, Novo Nordisk can find the issue on an Amazon QuickSight dashboard, review the analyzed image, and correct the error. Moreover, the company has complete traceability of the ML model in production, a necessity in the highly regulated pharmaceutical industry. \\xa0 After building out the pipeline to run its cartridge-counting model, Novo Nordisk wanted to see if it could repurpose it for a different use case for scalability. During the last 2 weeks of the prototyping engagement, the company configured the pipeline to detect bacteria growth on agar plates, thousands of which are manually analyzed every day. “We didn’t need to change much,” says Jonas Vejlgård Kristensen, solutions architect at Novo Nordisk. “We simply took a new dataset and used a different ML model. Then, we employed an anomaly-detection approach and adjusted the camera settings.” \\nLearn more\\xa0»\\nquality-assurance tasks \\n Amazon QuickSight\\nML models at scale to different edge devices \\nOverview | Opportunity | Solution | Outcome | AWS Services Used \\nNovo Nordisk had explored ML to automate time-consuming, manual tasks, but many of its processes were disconnected and difficult to scale. “We had all the parts of the ML-development process running locally on individual machines, from data processing to model training and even the manual transfer of the model to the edge devices,” says Carlos Ribera Codina, ML engineer at Novo Nordisk. “They were not interconnected, so this process could become quite difficult, especially when we had to deploy the models at scale and maintain them in production.” The team chose to migrate because it could use AWS services to create a pipeline that would run all these models automatically and interconnect them to expedite the development process. \\xa0 Novo Nordisk entered into a 6-week prototyping engagement with the AWS team to train and deploy an ML model that uses computer vision to count the number of drug cartridges in a box—a task that it previously performed manually and was time and resource intensive. The new process involved capturing images of cartridge boxes from above, using pre-trained models to detect a cartridge, and counting the number of locations where a cartridge is identified in an image. \\nPortuguês'} score: 0.1639048600394873\n"
     ]
    }
   ],
   "source": [
    "# Search time for awesome wines!\n",
    "\n",
    "hits = qdrant.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=encoder.encode(user_prompt).tolist(),\n",
    "    limit=3,\n",
    ")\n",
    "for hit in hits:\n",
    "    print(hit.payload, \"score:\", hit.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33243e5d-9e0d-4ec4-98e9-3fc56b8bdb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a variable to hold the search results\n",
    "search_results = [hit.payload for hit in hits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter a query\n",
    "query = \"Write a detailed summary of AWS Personalize and in your response provided an executive summary after which you can present succinct bullet points? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\"\n",
    "# query = \"Write a detailed summary of the use of Machine Learning by AWS customers in the Life Science industry. In your response provided an executive summary after which you can present succinct bullet points? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\"\n",
    "# query = \"Write a detailed summary of the companies using AWS IoT services. In your response provided an executive summary after which you can present succinct bullet points? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\"\n",
    "# query = \"Write a detailed summary of the competitive advantage of AWS services over other cloud vendors in the AI space. In your response provided an executive summary after which you can present succinct bullet points? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\"\n",
    "# query = \"\"Write a detailed summary how GenAI applications can be used in the Fintech industry. In your response provided an executive summary after which you can present succinct bullet points? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6c2b91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The article \"Novo Nordisk: Using AWS to streamline manufacturing processes and reduce manual labor\" by the author provides an overview of the company\\'s journey to adopting AWS for its manufacturing operations. The article highlights the company\\'s use of AWS services such as AWS IoT Greengrass, AWS S3, and AWS SageMaker Edge, which have enabled them to automate key tasks on manufacturing lines, enhancing quality, and improving yield. The article also discusses the company\\'s use of AWS IoT Greengrass to automate key tasks on manufacturing lines, including cartridge counting and anomaly detection for agar plates. The article concludes by highlighting the benefits of using AWS for Novo Nordisk\\'s manufacturing operations, including increased efficiency, reduced manual labor, and improved quality.</s>', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Now time to connect to the local large language model\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://127.0.0.1:8080/v1\", # \"http://<Your api-server IP>:port\"\n",
    "    api_key = \"sk-no-key-required\"\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"LLaMA_CPP\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are chatbot. Your priority is to help guide users.\"},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "        {\"role\": \"assistant\", \"content\": str(search_results)}\n",
    "    ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
